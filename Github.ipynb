{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# added packages\n",
    "import heapq\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "## Reinforcement learning\n",
    "\n",
    "Consider a **cube** state space defined by $0 \\le x, y, z \\le L$. Suppose you are piloting/programming a drone to learn how to land on a platform at the center of the $z=0$ surface (the bottom). Some assumptions:\n",
    "* In this discrete world, if I say the drone is at $(x,y,z)$ I mean that it is in the box centered at $(x,y,z)$. And there are boxes (states) centered at $(x,y,z)$ for all $0 \\le x,y,z \\le L$. Each state is a 1 unit cube. So when $L=2$ (for example), there are cubes centered at each $x=0,1,2$, $y=0,1,2$ and so on, for a total state space size of $3^3 = 27$ states.\n",
    "* All of the states with $z=0$ are terminal states.\n",
    "* The state at the center of the bottom of the cubic state space is the landing pad. So, for example, when $L=4$, the landing pad is at $(x,y,z) = (2,2,0)$.\n",
    "* All terminal states ***except*** the landing pad have a reward of -1. The landing pad has a reward of +1.\n",
    "* All non-terminal states have a reward of -0.01.\n",
    "* The drone takes up exactly 1 cubic unit, and begins in a random non-terminal state.\n",
    "* The available actions in non-terminal states include moving exactly 1 unit Up (+z), Down (-z), North (+y), South (-y), East (+x) or West (-x). In a terminal state, the training episode should end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A\n",
    "Write a class `MDPLanding` to represent the Markov decision process for this drone. Include methods for:\n",
    "1. `actions(state)`, which should return a list of all actions available from the given state\n",
    "2. `reward(state)`, which should return the reward for the given state\n",
    "3. `result(state, action)`, which should return the resulting state of doing the given action in the given state\n",
    "\n",
    "and attributes for:\n",
    "1. `states`, which is just a list of all the states in the state space, where each state is represented as an $(x,y,z)$ tuple\n",
    "2. `terminal_states`, a dictionary where keys are the terminal state tuples and the values are the rewards associated with those terminal states\n",
    "3. `default_reward`, which is a scalar for the reward associated with non-terminal states\n",
    "4. `all_actions`, a list of all possible actions (Up, Down, North, South, East, West)\n",
    "5. `discount`, the discount factor (use $\\gamma = 0.999$ for this entire problem)\n",
    "\n",
    "How you feed arguments/information into the class constructor is up to you.\n",
    "\n",
    "Note that actions are *deterministic* here.  The drone does not need to learn transition probabilities for outcomes of particular actions. What the drone does need to learn, however, is where the heck that landing pad is, and how to get there from any initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MDPLanding:\n",
    "    def __init__(self, L, default_reward, discount):\n",
    "        # states represent all positions in a 3D box\n",
    "        self.states = [(x,y,z) for x in range(0,L+1) for y in range(0,L+1) for z in range(0,L+1)]\n",
    "        # terminal states are all states where z = 0 (ground)\n",
    "        self.terminalStates = [(x,y,0) for x in range(0,L+1) for y in range(0,L+1)]\n",
    "        self.defaultReward = default_reward\n",
    "        self.discount = discount\n",
    "        self.L = L\n",
    "    \n",
    "    def actions(self, state):\n",
    "        actions = []\n",
    "        # trys going down first because the bottom is where all the valuable info is\n",
    "        if (state[0], state[1], state[2]-1) in self.states: # Down\n",
    "            actions.append((state[0], state[1], state[2]-1))\n",
    "        if (state[0]+1, state[1], state[2]) in self.states: # East\n",
    "            actions.append((state[0]+1, state[1], state[2]))\n",
    "        if (state[0]-1, state[1], state[2]) in self.states: # West\n",
    "            actions.append((state[0]-1, state[1], state[2]))\n",
    "        if (state[0], state[1]+1, state[2]) in self.states: # North\n",
    "            actions.append((state[0], state[1]+1, state[2]))\n",
    "        if (state[0], state[1]-1, state[2]) in self.states: # South\n",
    "            actions.append((state[0], state[1]-1, state[2]))\n",
    "        if (state[0], state[1], state[2]+1) in self.states: # Up\n",
    "            actions.append((state[0], state[1], state[2]+1))\n",
    "        return actions\n",
    "    \n",
    "    def reward(self, state):\n",
    "        if state in self.terminalStates:\n",
    "            # landing pad\n",
    "            if state == (self.L/2, self.L/2, 0): return 1\n",
    "            # wrong ground location\n",
    "            else: return -1\n",
    "        # anywhere in the air\n",
    "        else: return -0.01\n",
    "        \n",
    "    def randomNonTerminalState(self):\n",
    "        s = (0,0,0)\n",
    "        while s in self.terminalStates:\n",
    "            s = self.states[np.random.choice(len(self.states))]\n",
    "        return s\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B\n",
    "\n",
    "Code up a **Q-learning** agent/algorithm to learn how to land the drone. You can do this however you like, as long as you use the MDP class structure defined above.  \n",
    "\n",
    "Your code should include some kind of a wrapper to run many trials to train the agent and learn the Q values (see Section 22.3 in the textbook - page 803 might be of particular interest).  You also do not need to have a separate function for the actual \"agent\"; your code can just be a \"for\" loop within which you are refining your estimate of the Q values.\n",
    "\n",
    "From each training trial, save the cumulative discounted reward (utility) over the course of that episode. That is, add up all of $\\gamma^t R(s_t)$ where the drone is in state $s_t$ during time step $t$, for the entire sequence. I refer to this as \"cumulative reward\" because we usually refer to \"utility\" as the utility *under an optimal policy*.\n",
    "\n",
    "Some guidelines:\n",
    "* The drone should initialize in a random non-terminal state for each new training episode.\n",
    "* The training episodes should be limited to 50 time steps, even if the drone has not yet landed. If the drone lands (in a terminal state), the training episode is over.\n",
    "* You may use whatever learning rate $\\alpha$ you decide is appropriate, and gives good results.\n",
    "* There are many forms of Q-learning. You can use whatever you would like, subject to the reliability targets in Part D below.\n",
    "* Your code should return:\n",
    "  * The learned Q values associated with each state-action pair.\n",
    "  * The cumulative reward for each training trial. \n",
    "  * Anything else that might be useful in the ensuing analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "def QlearningAlgo(env, episodes):\n",
    "    # initialize Q and N dicts and cumulative reward list\n",
    "    Q = {}\n",
    "    N = {}\n",
    "    cumulativeReward = []\n",
    "    \n",
    "    # fill all dictionary values with 0\n",
    "    for initS in env.states:\n",
    "        for initA in env.actions(initS):\n",
    "            Q[initS,initA] = 0\n",
    "            N[initS,initA] = 0\n",
    "    \n",
    "    # run episodes\n",
    "    for e in range(episodes):\n",
    "        \n",
    "        # extend cumulativeReward list\n",
    "        cumulativeReward.append(0)\n",
    "        \n",
    "        # generate random state that is not a terminal state\n",
    "        s = env.randomNonTerminalState()\n",
    "\n",
    "        # run episode for 50 time steps\n",
    "        start = time.time()\n",
    "        timeSteps = 50\n",
    "        while time.time() < start + timeSteps:\n",
    "            \n",
    "            # pick best action from s based on Q values\n",
    "            # effectively moves the drone into state a\n",
    "            bestQ = -np.inf\n",
    "            for action in env.actions(s):\n",
    "                if Q[s,action] > bestQ:\n",
    "                    bestQ = Q[s,action]\n",
    "                    a = action\n",
    "\n",
    "            # calcluate exploratory motivator:\n",
    "            # increment number of visits to state a from state s.\n",
    "            # explore decreases as N[s,a] increases.\n",
    "            # explore provides a consequence for visiting states many times.\n",
    "            N[s,a] += 1\n",
    "            explore = 1 / N[s,a]\n",
    "\n",
    "            # find maximum Q value in current state a's actions\n",
    "            # how good of a position is the drone now in?\n",
    "            bestNextQ = -np.inf \n",
    "            for aPrime in env.actions(a):\n",
    "                bestNextQ = max(bestNextQ, Q[a,aPrime])\n",
    "\n",
    "            # update Q value:\n",
    "            # (previous value of moving from s to a) + (the new learned value of moving from s to a)\n",
    "            Q[s,a] = Q[s,a] + explore * (env.reward(a) + (env.discount * bestNextQ) - Q[s,a])\n",
    "\n",
    "            # record ammount of time passed in this episode\n",
    "            currentTimeStep = time.time() - start\n",
    "            \n",
    "            # cumulative reward = discount * the reward of the current state\n",
    "            # as currentTimeStep increases, the discount will decrease which will decrease reward more.\n",
    "            # as time passes, the rewards become more discounted. achieving goal sooner is more valuble than later. pretty cool.\n",
    "            cumulativeReward[e] += (env.discount**currentTimeStep) * env.reward(a)\n",
    "            \n",
    "            # if drone is in a terminal state (landed), episode is over\n",
    "            if a in env.terminalStates:\n",
    "                break\n",
    "            \n",
    "            # if drone still flying, set s to a and do it all over again\n",
    "            s = a\n",
    "    \n",
    "    # return dict of Q values and list of cumulative rewards for each episode\n",
    "    return Q,cumulativeReward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating environment and runnning the Q learning algo to train the drone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MDPLanding(L = 10, default_reward = -0.01, discount = .999)\n",
    "droneReportCard = QlearningAlgo(env = env, episodes = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Animation of drone using learned Q-values to land in coorect location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "# Returns a list of (x,y,z) tupels generated from Q Learning product\n",
    "def land_drone(s):\n",
    "    Q = droneReportCard[0]\n",
    "    path = []\n",
    "    bestQ = -np.inf\n",
    "    a = (1,1,1)\n",
    "    while a not in env.terminalStates:\n",
    "        for action in env.actions(s):\n",
    "            if Q[s,action] > bestQ:\n",
    "                bestQ = Q[s,action]\n",
    "                a = action\n",
    "        path.append(a)\n",
    "        s = a\n",
    "    return path\n",
    "\n",
    "colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'w']\n",
    "\n",
    "paths = [land_drone((env.randomNonTerminalState())) for _ in range(len(colors))]\n",
    "longest = max(len(l) for l in paths)\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "plt.rcParams['figure.dpi'] = 90\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(8, 8)\n",
    "lim = (0, 10)\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set(xlim3d=lim, ylim3d=lim, zlim3d=lim, xlabel='x', ylabel='y', zlabel='z')\n",
    "\n",
    "def animate(i):\n",
    "    for idx, p in enumerate(paths):\n",
    "        xs, ys, zs = zip(*p[:i+1])\n",
    "        ax.plot3D(xs, ys, zs, color=colors[idx])\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=longest)\n",
    "plt.close()\n",
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D\n",
    "\n",
    "Initialize the $L=10$ environment (so that the landing pad is at $(5,5,0)$). Run some number of training trials to train the drone.\n",
    "\n",
    "**How do I know if my drone is learned enough?**  If you take the mean cumulative reward across the last 5000 training trials, it should be around 0.80. This means at least about 10,000 (but probably more) training episodes will be necessary. It will take a few seconds on your computer, so start small to test your codes.\n",
    "\n",
    "**Then:** Compute block means of cumulative reward from all of your training trials. Use blocks of 500 training trials. This means you need to create some kind of array-like structure such that its first element is the mean of the first 500 trials' cumulative rewards; its second element is the mean of the 501-1000th trials' cumulative rewards; and so on. Make a plot of the block mean rewards as the training progresses. It should increase from about -0.5 initially to somewhere around +0.8.\n",
    "\n",
    "**And:** Print to the screen the mean of the last 5000 trials' cumulative rewards, to verify that it is indeed about 0.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Mean of last 5000 trials cumulative rewards: ', np.mean(droneReportCard[1]))\n",
    "\n",
    "blockMeans = np.array([])\n",
    "episodes = np.array([])\n",
    "\n",
    "for i in range(0, 5000, 500):\n",
    "    blockMeans = np.append(blockMeans, np.mean(droneReportCard[1][i : i+499]))\n",
    "    episodes = np.append(episodes, i+500)\n",
    "\n",
    "df = pd.DataFrame(data=blockMeans, columns=['Cumulative Rewards'])\n",
    "df['Episodes'] = episodes\n",
    "df.head()\n",
    "\n",
    "df.plot(x ='Episodes', y='Cumulative Rewards', kind = 'line', color = 'black')\n",
    "plt.scatter(x = df['Episodes'], y= df['Cumulative Rewards'], color = 'black')\n",
    "plt.title('Drone Training Performance')\n",
    "plt.ylabel('Cumulative Rewards')\n",
    "plt.grid(True)\n",
    "plt.xticks(np.arange(0, 5500, 500), rotation=-40)\n",
    "plt.yticks(np.arange(-1, 1.2, .2))\n",
    "plt.gcf().set_size_inches(10, 5)\n",
    "plt.legend().remove()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
